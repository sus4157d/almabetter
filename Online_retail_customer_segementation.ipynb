{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sus4157d/almabetter/blob/main/Online_retail_customer_segementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Online Retail Customer Segmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have been given  all transational data from 1-Dec-2010 to 9-Dec-2011 of a Uk based non-store online retail company . Customers of company are mostly wholesellers. We are required to identify the major customer segments. We started importing the library and loading CSV file in DataFrame df. We explored the how Head and Tail of df look like and checked the shape. we have 541909 row of datapoints with 8 columns. Out of Eight columns we have 'InvoiceNo','Stockcode','Description' as first three . Dtype of these is 'object', 'InvoiceNo' is unique transation No which have 'c' latter added if order was cancelled. we will drop all orders which were cancelled later on. Next is 'Stockcode' which is a unique code for each individual item used to identify in store. 'Description' is what is this item which was brought. 'InvoiceDate' gives Date and Time of transaction . Next are 'Quantity' which is int and tells how many items are brought . 'UnitPrice' is float tells what is price per item brought. 'CustomerID' is int again and its unique Id of each customer as we know. Last but not least we have 'Country' that tells us country name of customer . Description Check gives Min , Mean, Max values of \"Quantity\" , \"UnitPrice\" and \"CustomerID\" , we can say we have anomalies in first two as there is huge difference between 99percent and max value. We will take care of anomalies later on. We have 135 thousand missing value in CustomerID we cannot predict anything from such transation . So we need to drop these transation. From Duplicate value of 5 thousand we droped all keeping first. we droped all transation of invoice with 'c' which were cancelled.\n",
        "\n",
        "Now we will start with Ploting Distribution of Feature Variable , EDA , Data visualization and Data Wrangling . Fistly we have ploted Distribution of \"Quantity\" and \"UnitPrice\" , Both gives Highly Right Skewed Plot . We have applied some Transformation and after that it became normal. We created new Feature from invoice Date like mounth, time, day, day_name, month_name . From feature Quantity and UnitPrice we created TotalAmount of each Transation.Created RFM model called Recency , Frequency and Monetary Model which model the rating of recency of each customer , frequency of each customer and How much amount each customer spend. From these rating we can rate each customer according to value of R, F, M Feature. Next is Data visualization , we ploted top five Product according to sales . Top Countries according to sales and Top Stock also .\n",
        "We have ploted graph of Day_of_week, Hour_of_day and Month_of_year with respect to sales . We did Hypothesis testing , Do Recent Customer tends to spend more , Do Frequent Customer spend More or Less. Now lets start Featue Engineering like droping anomalies, Log Transformation and Scaling using StanderdScaler. Let start with Model Implementation , we have fitted K-Means, DBSCAN, and HIERICIAL Clustering technique with Silhouette score, elbow method and dendogram to find Optimum no of clusters.I finished with number two for optimum clusters using differnt method. lets begin the project."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/sus4157d/almabetter.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this Project We are required to Identify Major Customer Segments of the data set. we are given dataset of all the transation of Uk based non-store ONLINE RETAIL. This company mainly sells all occassion gifts and mostly the customer are whole seller.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from scipy.stats import *\n",
        "import statistics\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.cm as cm\n",
        "from seaborn.matrix import dendrogram\n",
        "\n",
        "import warnings\n",
        "from pylab import rcParams\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d5LfvGuDVyvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dfn=pd.read_excel(\"/content/drive/MyDrive/Online_Retail.xlsx\",parse_dates=['InvoiceDate'])"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a copy of Dataset\n",
        "df=dfn.copy()"
      ],
      "metadata": {
        "id": "wq4eDKQgeNcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "Pek6GVO_A3bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have Null values in CustomerID and which is importent feature and filling Null values affect our coustmer segementation. So we drop all null values datapoint. which wil give us a dataset of 406 thousand rows."
      ],
      "metadata": {
        "id": "At1Kh3X8aXnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Understanding Feature  Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlLxAtlziMbP"
      },
      "source": [
        "#### InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "#### StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "####Description: Product (item) name. Nominal.\n",
        "#### Quantity: The quantities of each product (item) per transaction. Numeric.\n",
        "#### InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "#### UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
        "#### CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "#### Country: Country name. Nominal, the name of the country where each customer resides."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataSet Description\n",
        "df.describe([0.99,0.90])"
      ],
      "metadata": {
        "id": "ZS9luAlgDBPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Quantity and UnitPrice have Max value very high in comparision to value at 99 percent which mean there could be outliers."
      ],
      "metadata": {
        "id": "yLctzuZvaVVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have null values in CustomerId and we cannt fill these values so we need to drop these values."
      ],
      "metadata": {
        "id": "65rS_uYInhoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(keep='first',inplace=True)"
      ],
      "metadata": {
        "id": "SXH3UA2TYJcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Canceled order"
      ],
      "metadata": {
        "id": "ZR4VVxShUyFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing datatype to str\n",
        "df[\"InvoiceNo\"]=df['InvoiceNo'].astype('str')"
      ],
      "metadata": {
        "id": "QlzD6SIcca_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Droping all the Invoice with 'C'\n",
        "df=df[~df['InvoiceNo'].str.contains('C')]"
      ],
      "metadata": {
        "id": "2uqjrV18cDKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Droping all the ordered which were canceled . Taking all the Invoice which dont contain 'c' latter"
      ],
      "metadata": {
        "id": "TFQ9eIvNU5NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Distribution of Features."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Distribution of Quantity\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['Quantity'],kde=True,color='red',bins=30)\n",
        "plt.title(\"Distribution of  Feature  Quantity \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is highly right skewed . We apply some transformation and check the result."
      ],
      "metadata": {
        "id": "yThvWBjlca6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####   Log Transformation of Distribution of Quantity."
      ],
      "metadata": {
        "id": "3TtPREw2cqjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Log Transformed Distribution of Quantity\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(np.log1p(df['Quantity']),kde=True,color='red',bins=30)\n",
        "plt.title(\"Distribution of  Feature  Quantity \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rwMDmU1QdDvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is Much Better then Earlier plot . Less skewed and similer to symmetric plot."
      ],
      "metadata": {
        "id": "RpmFwn8vdUIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Checking the Distribution of UnitPrice"
      ],
      "metadata": {
        "id": "_yxvNFPod2XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Distribution of UnitPrice\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['UnitPrice'],kde=True,color='red',bins=30)\n",
        "plt.title(\"Distribution of  Feature  UnitPrice \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pttUHt97fEHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the Distribution of UnitPrice is Hightly Right Skewed . We need to Transform the Distribution . We will apply Log Transformation ."
      ],
      "metadata": {
        "id": "8pCFVSPdfDLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Log Transformation of UnitPrice Distribution."
      ],
      "metadata": {
        "id": "CaJpST_df180"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Distribution of UnitPrice with Log1P Transformation\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(np.log1p(df['UnitPrice']),kde=True,color='red',bins=30)\n",
        "plt.title(\"Distribution of  Feature  UnitPrice \")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jc9DHt89gAqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is much much better , it is less skewed and more kind of Symmetric"
      ],
      "metadata": {
        "id": "52vGAVLqgZve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Creating new Feature From Invoice Date"
      ],
      "metadata": {
        "id": "BtRjWIqsg1WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating feature from Invoice Date\n",
        "df['Day_Name']=df.InvoiceDate.dt.day_name()\n",
        "df['Day_Month']=df.InvoiceDate.dt.day\n",
        "df['Hour']=df.InvoiceDate.dt.hour\n",
        "df['Month_Name']=df.InvoiceDate.dt.month_name()\n",
        "df['Date']=df.InvoiceDate.dt.date"
      ],
      "metadata": {
        "id": "yOJZsuALg0FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Feature 'TotalAmount' From Quantity and UnitPrice"
      ],
      "metadata": {
        "id": "PoWd2mrzm0YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating New Feature 'TotalAmount' from Quantity and UnitPrice\n",
        "df['TotalAmount']=df['Quantity']*df['UnitPrice']"
      ],
      "metadata": {
        "id": "1Ud3U8i2iyZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XgHqziqTwC5"
      },
      "source": [
        "### Creating the RFM model (Recency, Frequency,Monetary value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPguk7bT9F2"
      },
      "source": [
        "Recency, frequency, monetary value is a marketing analysis tool used to identify a company's or an organization's best customers by using certain measures. The RFM model is based on three quantitative factors: Recency: When was Last Produch Purchased. Frequency: How often a customer makes a purchase. Monetary Value: How much money a customer spends on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BJEmdTIUKFQ"
      },
      "source": [
        "Performing RFM Segmentation and RFM Analysis, Step by Step\n",
        "The first step in building an RFM model is to assign Recency, Frequency and Monetary values to each customer. ...\n",
        "The second step is to divide the customer list into tiered groups for each of the three dimensions (R, F and M), using Excel or another tool."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Creating the Recency Feature"
      ],
      "metadata": {
        "id": "4SRHPLBbq85n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Latest Date (Date of Last Purchase)\n",
        "Latest_Date=df['Date'].max()\n",
        "#Getting last purchase Date for each customer\n",
        "df_r=df.groupby(['CustomerID'],as_index=False)['Date'].max()\n",
        "#Getting the Recency\n",
        "df_r['Recency']=(Latest_Date-df_r['Date']).dt.days"
      ],
      "metadata": {
        "id": "qinKjP4Ops6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating Frequency Feature"
      ],
      "metadata": {
        "id": "rokONZYM486N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting Frequency\n",
        "df_f=df.groupby(['CustomerID'],as_index=False)['Date'].count()\n",
        "#Renaming Frequecy Coulmn\n",
        "df_f.rename(columns={'Date':'Frequency'},inplace=True)"
      ],
      "metadata": {
        "id": "CA-Jk2Jz5NjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating Monetary Feature"
      ],
      "metadata": {
        "id": "0n7wCvTU6VHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting Total Amount Spend by each Customer\n",
        "df_a=df.groupby(['CustomerID'],as_index=False).agg({'TotalAmount':'sum'})\n",
        "#Renaming the Monetary Function\n",
        "df_a.rename(columns={'TotalAmount':'Monetary'},inplace=True)"
      ],
      "metadata": {
        "id": "xsUraCnl7kdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Getting Merged Dataset"
      ],
      "metadata": {
        "id": "DeS0d5qrzgtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging all Data set of Recency Frequency and Monetary\n",
        "df_m=df_a.merge(df_f,on='CustomerID')\n",
        "df_rfm=df_m.merge(df_r,on=\"CustomerID\",how='right').drop(\"Date\",axis=1)\n",
        "df_rfm.head()"
      ],
      "metadata": {
        "id": "4KU4qLiW-D0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5owQ6D_uUSfa"
      },
      "source": [
        "#####Calculating RFM scores\n",
        "\n",
        "The number is typically 5 . If you decide to code each RFM attribute into 3 categories, you'll end up with 125 different coding combinations ranging from a high of 111 to a low of 555. Generally speaking, the Lower the RFM score, the more valuable the customer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cutting each Feature in Three section according to value and assigning labels\n",
        "df_rfm['R']=pd.qcut(df_rfm['Recency'],5,labels=[1,2,3,4,5])\n",
        "df_rfm['F']=pd.qcut(df_rfm['Frequency'],5,labels=[5,4,3,2,1])\n",
        "df_rfm['M']=pd.qcut(df_rfm['Monetary'],5,labels=[5,4,3,2,1])"
      ],
      "metadata": {
        "id": "peyak8OMniq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Grouped 'RMF' Feature\n",
        "df_rfm['RFM']=df_rfm['R'].astype('str')+df_rfm['F'].astype('str')+df_rfm['M'].astype('str')\n",
        "df_rfm.head()"
      ],
      "metadata": {
        "id": "QxvjCoAXkQb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***EDA And Data Visualization***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Highest Selling Product"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Top five product\n",
        "df_des=df['Description'].value_counts().sort_values(ascending=False)\n",
        "df_des.head()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Highest selling product\n",
        "plt.figure(figsize=(10,5))\n",
        "df_des.head().plot(kind='bar')\n",
        "plt.title(\"Highest selling product\")\n",
        "plt.xlabel(\"top five product\")\n",
        "plt.ylabel(\"No of product sold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CvMvlZ0crxC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***These are top five product according to sale . Highest selling product is WHITE HANGING HEART T-LIGHT HOLDER  and it sold  2016 times. followed by other product.***"
      ],
      "metadata": {
        "id": "NbxHkz6w11lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Least Selling Five Product"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Least selling five product\n",
        "df_des.tail()"
      ],
      "metadata": {
        "id": "toBGdRT86E7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Lowest selling product\n",
        "plt.figure(figsize=(10,5))\n",
        "df_des.tail().plot(kind='bar')\n",
        "plt.title(\"Lowest selling product\")\n",
        "plt.xlabel(\"Least selling product\")\n",
        "plt.ylabel(\"No of product\")"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This is least of least selling product . There are many product which are sold single time . I have included only five of them.***"
      ],
      "metadata": {
        "id": "VaTEAYLp2nLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Country Wise Sales"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Countries Five Top By Sales\n",
        "df_cou=df.Country.value_counts()\n",
        "df_cou.head()"
      ],
      "metadata": {
        "id": "k96jcPmtFqpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting Top five Country by Sales\n",
        "plt.figure(figsize=(10,5))\n",
        "df_cou.head().plot(kind='bar')\n",
        "plt.title(\"Top five Country By Sales\")\n",
        "plt.xlabel(\"Countries\")\n",
        "plt.ylabel(\"Sales Count\")"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Top Country according to sales is United Kingdom . Which have nearly 90 percent share . Followed by other Europian Countries like Germany , France and Spain.***"
      ],
      "metadata": {
        "id": "7O2BIhRk3XAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Last Five Country By Sales\n"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Country with Least Sales\n",
        "df_cou.tail()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting Last Five Country By Sales\n",
        "plt.figure(figsize=(10,5))\n",
        "df_cou.tail().plot(kind='bar')\n",
        "plt.title('Last Five Country By Sales')\n",
        "plt.xlabel('Countries')\n",
        "plt.ylabel('Sales Count')"
      ],
      "metadata": {
        "id": "S9AatX7EG_4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Lithuania,\n",
        "Brazil ,\n",
        "Czech Republic,\n",
        "Bahrain, and\n",
        "Saudi Arabia are last in list of sales , there could be any reason for it , might be company recently started selling product to these countries .***"
      ],
      "metadata": {
        "id": "vMJzbPFD4Bjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Top Five Stock Counts"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Top Five Stock usedand there count\n",
        "df_st=df.StockCode.value_counts()\n",
        "df_st.head()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting Top Five Stock Code\n",
        "plt.figure(figsize=(10,5))\n",
        "df_st.head().plot(kind='bar')\n",
        "plt.title(\"Top Five Stock Code\")\n",
        "plt.xlabel(\"Stock Code\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_zPe-BxEJ12t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Each stock represent a particular item or location , these are highest used stock like 85123A is used more than two thousand times.***"
      ],
      "metadata": {
        "id": "0-6Bi7am5AgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Bottom Five Stock Counts"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bottom Five Stock Counts\n",
        "df_st.tail()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting Bottom Five Stock Counts\n",
        "plt.figure(figsize=(10,5))\n",
        "df_st.tail().plot(kind='bar')\n",
        "plt.title(\"Bottom Five Stock Code\")\n",
        "plt.xlabel(\"Stock Code\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GuNWfpt5LnIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Orders During Day"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot of orders' time of day\n",
        "plt.figure(figsize=(10,5))\n",
        "df.groupby(\"Hour\")[\"Hour\"].count().plot(kind='line')\n",
        "plt.title('Time of order')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel(\"No of orders\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We can see that most of orders recieved during day time from 8am to 5pm . There are negligible order recieved during morning and evening time***"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sales During Month"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the Sales Count During Month\n",
        "plt.figure(figsize=(12,5))\n",
        "df.groupby(\"Day_Month\")['Day_Month'].count().plot(kind='line')\n",
        "plt.title(\"Sales Count During Month\")\n",
        "plt.xlabel(\"Day of Month\")\n",
        "plt.ylabel(\"Sales Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZxZx3oKXT3pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We can see that count of sales is high at begining of month and low at end of month which might be the reason that at starting of month we get salery and tends to spend but last of month our tendency is to save money.***"
      ],
      "metadata": {
        "id": "2oPOJfMgeav1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Purchasing Day Count"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the Sales Count During Month\n",
        "plt.figure(figsize=(12,5))\n",
        "df.groupby(\"Day_Name\")['Day_Name'].count().sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title(\"Sales Count During week\")\n",
        "plt.xlabel(\"Day of Week\")\n",
        "plt.ylabel(\"Sales Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We observed More Sales on Thrusday. And least sales is observed on Friday.***"
      ],
      "metadata": {
        "id": "_JOL-OWHXLG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Purchasing During Year"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Month_Name\")['Month_Name'].count().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "MBrbuUexXvRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the Sales Count During Year\n",
        "plt.figure(figsize=(12,5))\n",
        "df.groupby(\"Month_Name\")['Month_Name'].count().sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title(\"Sales Count During Year\")\n",
        "plt.xlabel(\"Month of Year\")\n",
        "plt.ylabel(\"Sales Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Highest Sales were observed in November, October and December Month and lowest were observed in January and February. The reason could be any thing may be because of Season , or Festival Season or some other occassion.***"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1: :Do Recent Customer Tends to Spend More.\n",
        "\n",
        "Hypothesis 2: Do Frequent Customer Spend Large or Small Amount at a Time.\n"
      ],
      "metadata": {
        "id": "Aan7lDcgG9bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Creating Parameter Class"
      ],
      "metadata": {
        "id": "8z07_u6JVs4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Parameter Class\n",
        "class findz:\n",
        "  def proportion(self,sample,hyp,size):\n",
        "    return (sample - hyp)/math.sqrt(hyp*(1-hyp)/size)\n",
        "  def mean(self,hyp,sample,size,std):\n",
        "    return (sample - hyp)*math.sqrt(size)/std\n",
        "  def varience(self,hyp,sample,size):\n",
        "    return (size-1)*sample/hyp\n",
        "\n",
        "variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)\n",
        "zcdf = lambda x: norm(0,1).cdf(x)\n",
        "\n",
        "# Creating a function for getting P value\n",
        "def p_value(z,tailed,t,hypothesis_number,df,col):\n",
        "  if t!=\"true\":\n",
        "    z=zcdf(z)\n",
        "    if tailed=='l':\n",
        "      return z\n",
        "    elif tailed == 'r':\n",
        "      return 1-z\n",
        "    elif tailed == 'd':\n",
        "      if z>0.5:\n",
        "        return 2*(1-z)\n",
        "      else:\n",
        "        return 2*z\n",
        "    else:\n",
        "      return np.nan\n",
        "  else:\n",
        "    z,p_value=stats.ttest_1samp(df[col],hypothesis_number)\n",
        "    return p_value\n",
        "\n",
        "  # Conclusion about the P - Value\n",
        "def conclusion(p):\n",
        "  significance_level = 0.05\n",
        "  if p>significance_level:\n",
        "    return f\"Failed to reject the Null Hypothesis for p = {p}.\"\n",
        "  else:\n",
        "    return f\"Null Hypothesis rejected Successfully for p = {p}\"\n",
        "\n",
        "# Initializing the class\n",
        "findz = findz()"
      ],
      "metadata": {
        "id": "R3RCGfD1HMr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 1. Do Recent Customer Tends to Spend More"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: TotalAmount_R <= TotalAmount_avg\n",
        "\n",
        "Alternate Hypothesis : TotalAmount_R >  TotalAmount_avg\n",
        "\n",
        "Test Type: right Tailed Test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypothesis_number=df['TotalAmount'].mean()\n",
        "df_hypo=df[df['Date']>pd.to_datetime(20111118, format='%Y%m%d')]\n",
        "sample_mean=df_hypo['TotalAmount'].mean()\n",
        "size=len(df_hypo)\n",
        "std=(statistics.variance(df_hypo['TotalAmount']))**0.5"
      ],
      "metadata": {
        "id": "PkxwvNggK3OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean,size,std)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='r',t=\"true\",hypothesis_number=hypothesis_number,df=df_hypo,col=\"TotalAmount\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "kZRQhS2VZ53g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used T-Test as the statistical testing to obtain P-Value and found the result that we failed to  rejected the Null hypothesis , can say that Recent Transaction doesnt spend Large  Amount in comparisation to average spending per transaction."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=df_hypo[\"TotalAmount\"].mean()- df_hypo[\"TotalAmount\"].median()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "ts__5CML2ukK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we can see median is greater than mean over 10. So, the distribution is postively skewed. For a skewed data Z-Test can't be performed.\n",
        "\n",
        "Non-parametric tests are most useful for small studies. Using non-parametric tests in large studies may provide answers to the wrong question, thus confusing readers. For studies with a large sample size, t-tests and their corresponding confidence intervals can and should be used even for heavily skewed data.\n",
        "\n",
        "So, for a skewed data we can use T-test for better result. Thus, I used t - test."
      ],
      "metadata": {
        "id": "O_fGrleOup2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hypothesis 2. Do Frequent Customer  Spend Large or Small Amount at a Time."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: TotalAmount_invoice_freq == TotalAmount_invoice_avg\n",
        "\n",
        "Alternate Hypothesis : TotalAmount_invoice_freq != TotalAmount_invoice_avg\n",
        "\n",
        "Test Type: Two Tailed Test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypothesis_number=df['TotalAmount'].mean()\n",
        "df_hypo2=df.merge(df_rfm[df_rfm.F.astype(int)==1]['CustomerID'] , on='CustomerID', how='inner')\n",
        "sample_mean=df_hypo2['TotalAmount'].mean()\n",
        "size=len(df_hypo2)\n",
        "std=(statistics.variance(df_hypo2['TotalAmount']))**0.5"
      ],
      "metadata": {
        "id": "kY5qkfa0WI-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean,size,std)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='d',t=\"true\",hypothesis_number=hypothesis_number,df=df_hypo2,col=\"TotalAmount\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "7vvos_m_WI-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used T-Test as the statistical testing to obtain P-Value and found the result that Null hypothesis has been rejected successfully in favour  of Alternative hypothesis and Frequent Customer spend Large or Small Amount at a time."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=df_hypo2[\"TotalAmount\"].mean()- df_hypo2[\"TotalAmount\"].median()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "sghRHidgv1qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we can see median is greater than mean over 10. So, the distribution is postively skewed. For a skewed data Z-Test can't be performed.\n",
        "\n",
        "Non-parametric tests are most useful for small studies. Using non-parametric tests in large studies may provide answers to the wrong question, thus confusing readers. For studies with a large sample size, t-tests and their corresponding confidence intervals can and should be used even for heavily skewed data.\n",
        "\n",
        "So, for a skewed data we can use T-test for better result. Thus, I used t - test."
      ],
      "metadata": {
        "id": "CQwGZGHGvvXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDl_KBum3au7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Droping the Outliers"
      ],
      "metadata": {
        "id": "kaiA6yvatMCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting all the Continuous Variable from the Dataset\n",
        "cont_var=['Quantity','UnitPrice']"
      ],
      "metadata": {
        "id": "kU438tgwsrf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution and Boxplot\n",
        "for var in cont_var:\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    fig = sns.boxplot(y=df[var])\n",
        "    fig.set_title('Box Plot of %s'%var)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    fig = sns.distplot(df[var])\n",
        "    plt.title(\"Distribution Plot of %s\"%var)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gC95K69JQHV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is Highly Right Skewed and there are outliers in it. we use IsolationForest tree based algorithm for anomaly detection ."
      ],
      "metadata": {
        "id": "-DJrz-B3o7fQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVAhj8ILr57l"
      },
      "source": [
        "#Using IsolationForest For Predition of Outliers of Anomaly in Data\n",
        "clf = IsolationForest(n_estimators=100, contamination=0.01, random_state=0)\n",
        "clf.fit(df[cont_var])\n",
        "\n",
        "# predict raw anomaly score\n",
        "df['multivariate_anomaly_score'] = clf.decision_function(df[cont_var])\n",
        "\n",
        "# prediction of a datapoint category outlier or inlier\n",
        "df['multivariate_outlier'] = clf.predict(df[cont_var])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction of a datapoint category outlier or inlier\n",
        "df['multivariate_outlier'].value_counts()"
      ],
      "metadata": {
        "id": "_wV2DaI6r57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We predicted anomalies using Isolation Forest . Anomalies are 3905 in number and we drop these anomalies ."
      ],
      "metadata": {
        "id": "1WeyvYCVsv0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We found 3905 outliear which we are going to take care\n",
        "df=df[df['multivariate_outlier']==1]"
      ],
      "metadata": {
        "id": "8qyYZb96r57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"multivariate_outlier\",\"multivariate_anomaly_score\"],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "J8XplQnor57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Monetary Feature"
      ],
      "metadata": {
        "id": "Rn8Yb06X38Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution of Monetary\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df_rfm['Monetary'],kde=True,bins=50)\n",
        "plt.title(\"Distribution of Monetary\")"
      ],
      "metadata": {
        "id": "0RerIrUZ0w3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that data is highly Right Skewed. We apply Log1p Transformation ."
      ],
      "metadata": {
        "id": "ca6xLYBw12lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution of Monetary\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(np.log1p(df_rfm['Monetary']),kde=True,bins=50)\n",
        "plt.title(\"Distribution of Monetary\")"
      ],
      "metadata": {
        "id": "KpICAxFv1w6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph looks much better Now so we apply Log Transformation to Monetary Feature."
      ],
      "metadata": {
        "id": "UTxgrarW3BuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Appling Log Transformation\n",
        "df_rfm['Monetary']=np.log1p(df_rfm['Monetary'])"
      ],
      "metadata": {
        "id": "Nrc7oScG3nii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Recency Feature"
      ],
      "metadata": {
        "id": "LneIE4Pj4Gbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution of Recency\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df_rfm['Recency'],kde=True,bins=50)\n",
        "plt.title(\"Distribution of Recency\")"
      ],
      "metadata": {
        "id": "dn5gHlDB4OTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph is Right Skewed and We need to apply Transformation to make it symmetric"
      ],
      "metadata": {
        "id": "6ar-037N415I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution of Log Transformation of Recency\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(np.log1p(df_rfm['Recency']),kde=True,bins=50)\n",
        "plt.title(\"Distribution of Recency\")"
      ],
      "metadata": {
        "id": "67Ku0Mk44Tq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Looks Much better then Before so we apply Log Transformation to it."
      ],
      "metadata": {
        "id": "jcDZR5EJ4vuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Appling Log Transformation\n",
        "df_rfm['Recency']=np.log1p(df_rfm['Recency'])"
      ],
      "metadata": {
        "id": "8korCX9K5JWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Frequency Feature"
      ],
      "metadata": {
        "id": "GjG2W3s85jPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution Frequency\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot((df_rfm['Frequency']),kde=True,bins=50)\n",
        "plt.title(\"Distribution of Frequency\")"
      ],
      "metadata": {
        "id": "WFBCNfW75nFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot is again Highly Right Skewed. We apply log Transformation to it."
      ],
      "metadata": {
        "id": "LdHwTpxv59SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the distribution of Log Transformation of Frequency\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(np.log1p(df_rfm['Frequency']),kde=True,bins=50)\n",
        "plt.title(\"Distribution of Frequency\")"
      ],
      "metadata": {
        "id": "831lh5OC6U00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got some Improvemnt in Distribution of Graph so we apply Log Transformation to it."
      ],
      "metadata": {
        "id": "mUvVdtB76e4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rfm['Frequency']=np.log1p(df_rfm['Frequency'])"
      ],
      "metadata": {
        "id": "R3FBPwr86t7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Feature list Which we want to Scale\n",
        "rfm_var=['Recency','Frequency','Monetary']"
      ],
      "metadata": {
        "id": "E8JRWUZi42XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Standard Scaler to create instance\n",
        "scaler=StandardScaler()\n",
        "#Fitting and Transforming the DATA\n",
        "df_rfm[rfm_var]=scaler.fit_transform(df_rfm[rfm_var])"
      ],
      "metadata": {
        "id": "N22TBC334Fhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We have applied StanderdScalar to scale the Recency,Frequency and Monetary.***"
      ],
      "metadata": {
        "id": "YHPwbbIX_RHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***CUSTOMER SEGMENTATION MODEL IMPLIMENTATION***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-MEANS Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Silhouette Score to get optimum no of clusters."
      ],
      "metadata": {
        "id": "-qeiaVzwIokf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting Feature Variable\n",
        "x=df_rfm[rfm_var].values\n",
        "#Getting Silhouette Score for clusters\n",
        "for cluster in range(2,15):\n",
        "  clu=KMeans(n_clusters=cluster)\n",
        "  #using K-Means for predition\n",
        "  preds=clu.fit_predict(x)\n",
        "  #Scoring with silhouette\n",
        "  score=silhouette_score(x,preds)\n",
        "  # Create a subplot with 1 row and 2 columns\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  fig.set_size_inches(18, 7)\n",
        "  ax1.set_xlim([-0.1, 1])\n",
        "  ax1.set_ylim([0, len(x) + (cluster + 1) * 10])\n",
        "  sample_silhouette_values=silhouette_samples(x,preds)\n",
        "  y_lower = 10\n",
        "  for i in range(cluster):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[preds == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / cluster)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10\n",
        "  ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "  ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "  ax1.set_ylabel(\"Cluster label\")\n",
        "  # The vertical line for average silhouette score of all the values\n",
        "  ax1.axvline(x=score, color=\"red\", linestyle=\"--\")\n",
        "  ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "  # 2nd Plot showing the actual clusters formed\n",
        "  colors = cm.nipy_spectral(preds.astype(float) /cluster)\n",
        "  ax2.scatter(x[:, 0], x[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "  # Labeling the clusters\n",
        "  centers = clu.cluster_centers_\n",
        "  # Draw white circles at cluster centers\n",
        "  ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "  for i, c in enumerate(centers):\n",
        "      ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "      ax2.set_title(\"The visualization of the clustered data.\")\n",
        "      ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "      ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "      plt.suptitle(f\"Silhouette analysis for KMeans clustering with n_clusters = {cluster} with score {score}\",fontsize=14, fontweight='bold')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. We got highest selhouette score .39 for no_of_clusters =2 , after that score is decreasing. So we can take two as cluster no and fit our data on it.***"
      ],
      "metadata": {
        "id": "THLFLRVOHJ6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Appling Elbow Method to get optimum no of clusters"
      ],
      "metadata": {
        "id": "_NcW5thYI2Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing the cluster no. and  square of distance\n",
        "sum_of_squares={}\n",
        "for cluster in range(2,15):\n",
        "  #K-Means for calcuating the SQUARE Distance\n",
        "  clu=KMeans(n_clusters=cluster,init='k-means++',max_iter=1000)\n",
        "  kl=clu.fit(x)\n",
        "  #Adding to Dictionary file\n",
        "  sum_of_squares[cluster]=kl.inertia_\n",
        "\n",
        "#Ploting the ELBOW GRAPH\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.pointplot(x=list(sum_of_squares.keys()),y=list(sum_of_squares.values()))\n",
        "plt.title(\"Elbow Method for Optimum Clusters\")\n",
        "plt.xlabel(\"No. of Clusters\")\n",
        "plt.ylabel(\"Sum of Square Distance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bg4sisV5I_TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Optimum No of Clusters is 3 to 5 using elbow method. we will try and fit data on these.***"
      ],
      "metadata": {
        "id": "4s8y23La5X4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***Predition of Clusters***"
      ],
      "metadata": {
        "id": "s7dB7TBdqYxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting The Clusters Predition with n=2\n",
        "clu=KMeans(n_clusters=2)\n",
        "cl=clu.fit_predict(x)\n",
        "cent=clu.cluster_centers_"
      ],
      "metadata": {
        "id": "DUbvEQTBmjsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means Clustering is an Unsupervised Learning algorithm, which groups the unlabeled dataset into different clusters. Here K defines the number of pre-defined clusters that need to be created in the process, as if K=2, there will be two clusters, and for K=3, there will be three clusters, and so on.It allows us to cluster the data into different groups and a convenient way to discover the categories of groups in the unlabeled dataset on its own without the need for any training.It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
        "\n",
        "The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats the process until it does not find the best clusters. The value of k should be predetermined in this algorithm."
      ],
      "metadata": {
        "id": "wOp66nIbE34O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3D Visualization of Clustering."
      ],
      "metadata": {
        "id": "Fn8ojDDPjZRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the 3D Graph of clustering\n",
        "fig=plt.figure(figsize=(15,10))\n",
        "plt.title(\"3D Visual of Clustering\")\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "xs=df_rfm['Recency']\n",
        "ys=df_rfm['Frequency']\n",
        "zs=df_rfm['Monetary']\n",
        "ax.scatter(xs,ys,zs,c=cl,s=5,cmap='copper')\n",
        "plt.scatter(cent[:,0],cent[:,1],c='red',s=300,alpha=.9)\n",
        "ax.set_xlabel(\"Recency\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_zlabel(\"Monetary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ok3IEFBwjkpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN Clustering"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Density Based Clustering Technique\n",
        "dbs=DBSCAN(eps=.5,min_samples=15)\n",
        "y_db=dbs.fit_predict(x)"
      ],
      "metadata": {
        "id": "Vs3WIO290U4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the 3D Graph of clustering\n",
        "fig=plt.figure(figsize=(15,10))\n",
        "plt.title(\"3D Visual of Clustering\")\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "xs=df_rfm['Recency']\n",
        "ys=df_rfm['Frequency']\n",
        "zs=df_rfm['Monetary']\n",
        "ax.scatter(xs,ys,zs,c=y_db,s=5,cmap='copper')\n",
        "ax.set_xlabel(\"Recency\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_zlabel(\"Monetary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uAH6Jrah9Ud5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density.\n",
        "\n",
        "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a base algorithm for density-based clustering. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers."
      ],
      "metadata": {
        "id": "Ca19FLwQF0nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HIERARCHICAL Clustering"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Dendogram to find Optimum No of Clusters"
      ],
      "metadata": {
        "id": "yxmVxxDW0e_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this algorithm, we develop the hierarchy of clusters in the form of a tree, and this tree-shaped structure is known as the dendrogram."
      ],
      "metadata": {
        "id": "n7TPcC58HzSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting the Dendogram\n",
        "plt.figure(figsize=(15,8))\n",
        "dendrogram=sch.dendrogram(sch.linkage(x, method='ward'))\n",
        "plt.title(\"Dendogram graph\")\n",
        "plt.xlabel(\"Customer\")\n",
        "plt.ylabel(\"Eculidean distance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***By Finding largest Vertical Distance without crossing horizontal line. Optimum no of clusters is no of vertical line being crossed by drawing a horigontal line at 90. Which gives 2.***"
      ],
      "metadata": {
        "id": "rRt04EUN2i4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting Agglomerative Clustering"
      ],
      "metadata": {
        "id": "NhoHEIx63zUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using agglomerative cluster\n",
        "agl=AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='ward')\n",
        "y_hc=agl.fit_predict(x)"
      ],
      "metadata": {
        "id": "ZOUSCWV_4USA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Agglomerative is a bottom-up approach, in which the algorithm starts with taking all data points as single clusters and merging them until one cluster is left.***"
      ],
      "metadata": {
        "id": "Y3IaAe1LHf5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ploting the Clustering on 3d map"
      ],
      "metadata": {
        "id": "BV6ZztmY8eyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ploting the 3D Graph of clustering\n",
        "fig=plt.figure(figsize=(15,10))\n",
        "plt.title(\"3D Visual of Clustering\")\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "xs=df_rfm['Recency']\n",
        "ys=df_rfm['Frequency']\n",
        "zs=df_rfm['Monetary']\n",
        "ax.scatter(xs,ys,zs,c=y_hc,s=5,cmap='copper')\n",
        "ax.set_xlabel(\"Recency\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_zlabel(\"Monetary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uqi4TLk-7Vv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***As we have trained our model successfully, now we can visualize the clusters corresponding to the dataset.Here we will use the same lines of code as we did in k-means clustering, except one change. Here we will not plot the centroid that we did in k-means, because here we have used dendrogram to determine the optimal number of clusters.***"
      ],
      "metadata": {
        "id": "4NG9VpgGIhdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Specify the Column Names while initializing the Table\n",
        "myTable = PrettyTable(['SL No.',\"Model_Name\",'Data', \"Optimal_Number_of_cluster\"])\n",
        "\n",
        "# Add rows\n",
        "myTable.add_row(['1',\"K-Means with silhouette_score \", \"RFM\", \"2\"])\n",
        "myTable.add_row(['2',\"K-Means with Elbow methos  \", \"RFM\", \"3\"])\n",
        "myTable.add_row(['3',\"Hierarchical clustering  \", \"RFM\", \"2\"])\n",
        "myTable.add_row(['4',\"DBSCAN \", \"RFM\", \"2\"])\n",
        "print(myTable)"
      ],
      "metadata": {
        "id": "CHK_GabQ_kF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have gone through all major clustering technique . With K-Means we predicted for range of 2 to 15 and calcualted the silhouette score and find out that cluster_no 2 is most scored. Next we ploted Elbow graph which was giving cluster_no 3 as optimum no of clusters. We finished taking cluster_no 2 as optimum and predicted the clusters and ploted 3d graph .\n",
        "Next we have choosen Density based clustering technique. DBSCAN dont require value of clusters before fitting . We predicted the clusters with dbscan and found optimum no of clusters as two.\n",
        "And at the end I have tried Hierarchical clustering technique. initially ploted dendogram , which is kind of tree type structure to show distance , we concluded that cluster_no 2 as optimum no of clusters by looking vertical distance . We fitted data on Aggloromative which is bottom up approach and where we started with single data point clusters and reached at end with two clusters as optimum no."
      ],
      "metadata": {
        "id": "a3BKJVwcRskl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this I conclude , Thanks for reading\n",
        "\n",
        " SUSHIL."
      ],
      "metadata": {
        "id": "3zUsJ984V1Z-"
      }
    }
  ]
}